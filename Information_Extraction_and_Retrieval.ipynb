{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/Elshamysamira/Information-Extraction-and-Retrieval/blob/main/Information_Extraction_and_Retrieval.ipynb",
      "authorship_tag": "ABX9TyPPCYqkjmuIGo6IW8uKg6em",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elshamysamira/Machine-Learning-Methods/blob/sami/Information_Extraction_and_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install chardet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDSYs0Vm7D_3",
        "outputId": "29211fe1-0b97-4b31-f5b3-d8d4312061ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "import chardet\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "kqGWMCxR6Ufg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')  # This downloads necessary datasets for tokenization"
      ],
      "metadata": {
        "id": "S3c_RbXjfnwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e915f0df-78dc-44a7-a96d-62a5a22ad9c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "yZMJ0xkC3eQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecfe3fe-f1c6-4c94-8bae-0a821c8d8307"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/My Drive/Documents'\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "def printing_of_file(filepath):\n",
        "    for file in files:\n",
        "        print(file)\n",
        "\n",
        "printing_of_file(files)"
      ],
      "metadata": {
        "id": "ta-LWrDI4cVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278e74a5-701c-4683-cd37-602bb4d36325"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Business Hints for Men and Women by A. R. Calhoun.txt\n",
            "The Young Man in Business by Edward William Bok.txt\n",
            "Creating Capital by Frederick L. Lipman.txt\n",
            "Fundamentals of Prosperity- What They Are and Whence They Come by Roger Ward Babson.txt\n",
            "The Knack of Managing by Lewis K. Urquhart and Herbert Watson.txt\n",
            "Dumbells of Business by Louis Custer Martin Reed.txt\n",
            "Forging Ahead in Business by Alexander Hamilton Institute.txt\n",
            "A thousand ways to make money.txt\n",
            "Increasing Human Efficiency in Business by Walter Dill Scott.txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration, v. 02 (of 10).txt\n",
            "Confessions of a Tradesman by Frank Thomas Bullen.txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration.txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration, v. 04 (of 10).txt\n",
            "Business Administration- Theory, Practice and Application.txt\n",
            "Up To Date Business by Seymour Eaton.txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration, v. 01 (of 10).txt\n",
            "Twenty Years of Hus'ling by J. P. Johnston.txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration, v. 03 (of 10).txt\n",
            "Fifty years in Wall Street by Henry Clews.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_books = [Path(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]"
      ],
      "metadata": {
        "id": "f2VrhmfNDZfs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZATION**"
      ],
      "metadata": {
        "id": "KXajX789d_O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######TOKENIZE HERE"
      ],
      "metadata": {
        "id": "SJTBwOeGeHFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_save_documents(documents, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)  # Create the output directory if it doesn't exist\n",
        "\n",
        "    tokenized_docs = {}\n",
        "    for documentID, document_path in enumerate(documents):\n",
        "        # Open and read the document content\n",
        "        with open(document_path, 'r', encoding='utf-8') as file:\n",
        "            document_content = file.read()\n",
        "\n",
        "        # Tokenize the document content\n",
        "        tokens = word_tokenize(document_content)\n",
        "        tokenized_docs[documentID] = tokens\n",
        "\n",
        "        # Save the tokenized content to a file\n",
        "        output_file_path = os.path.join(output_dir, f\"tokenized_document_{documentID}.txt\")\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.write(' '.join(tokens))  # Write tokens separated by spaces\n",
        "\n",
        "    return tokenized_docs\n"
      ],
      "metadata": {
        "id": "hVAzCxJ1T9El"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_directory = '/content/drive/My Drive/Documents'\n",
        "\n",
        "# Tokenize documents and save the results\n",
        "tokenized_books = [os.path.join('/content/drive/My Drive/Documents', f'tokenized_document_{i}.txt') for i in range(len(tokenize_and_save_documents(all_books, output_directory)))]\n",
        "\n",
        "print(\"Tokenization complete and files saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKMIUKeFUKXa",
        "outputId": "f9892d1e-c745-41f5-abc1-5248996f2534"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization complete and files saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inverted Index and Pickling"
      ],
      "metadata": {
        "id": "RV5dcOQwXjyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the inverted index as a defaultdict of sets\n",
        "inverted_index = defaultdict(set)\n",
        "\n",
        "for documentID, document_path in enumerate(tokenized_books):\n",
        "    with open(document_path, 'r', encoding='utf-8') as file:\n",
        "        document_content = file.read()\n",
        "\n",
        "    # Tokenize by splitting on whitespace and standardize to lowercase\n",
        "    for word in document_content.lower().split():\n",
        "        print(documentID, os.path.basename(document_path), word)\n",
        "        inverted_index[word].add(documentID)  # Use set to avoid duplicate document IDs\n",
        "\n",
        "# Convert sets to lists if a list representation is preferred\n",
        "for word in inverted_index:\n",
        "    inverted_index[word] = list(inverted_index[word])\n",
        "\n",
        "# Save the inverted index using pickle\n",
        "with open('/content/drive/My Drive/Documents/inverted_index.pkl', 'wb') as f:\n",
        "    pickle.dump(inverted_index, f)\n",
        "\n",
        "#print(\"Inverted index has been saved successfully.\")\n"
      ],
      "metadata": {
        "id": "srcZeqTaJcT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all_books is a list of Path objects\n",
        "document_mapping = {documentID: document.name for documentID, document in enumerate(all_books)} #WE MUST CHANGE THIS CODE HERE TO ACCESS THE PICKLED VERSION"
      ],
      "metadata": {
        "id": "FGgDBLdkL5qN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lookup_word(word):\n",
        "    if word in inverted_index:\n",
        "        document_ids = inverted_index[word]\n",
        "        print(f\"Congratulations! I could find the word '{word}' in the following document ID(s): {document_ids}\")\n",
        "        for doc_id in document_ids:\n",
        "            print(f\"Document ID: {doc_id}, Document Name: {document_mapping[doc_id]}\")\n",
        "    else:\n",
        "        print(f\"I'm sorry, I couldn't find the word '{word}' in any document.\")"
      ],
      "metadata": {
        "id": "NDtUp88wEiaL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_word(\"should\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edXAlaKUKmy0",
        "outputId": "f10044f2-e4d0-45d1-c58d-c9920794dd1e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Congratulations! I could find the word 'should' in the following document ID(s): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
            "Document ID: 0, Document Name: Business Hints for Men and Women by A. R. Calhoun.txt\n",
            "Document ID: 1, Document Name: The Young Man in Business by Edward William Bok.txt\n",
            "Document ID: 2, Document Name: Creating Capital by Frederick L. Lipman.txt\n",
            "Document ID: 3, Document Name: Fundamentals of Prosperity- What They Are and Whence They Come by Roger Ward Babson.txt\n",
            "Document ID: 4, Document Name: The Knack of Managing by Lewis K. Urquhart and Herbert Watson.txt\n",
            "Document ID: 5, Document Name: Dumbells of Business by Louis Custer Martin Reed.txt\n",
            "Document ID: 6, Document Name: Forging Ahead in Business by Alexander Hamilton Institute.txt\n",
            "Document ID: 7, Document Name: A thousand ways to make money.txt\n",
            "Document ID: 8, Document Name: Increasing Human Efficiency in Business by Walter Dill Scott.txt\n",
            "Document ID: 9, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration, v. 02 (of 10).txt\n",
            "Document ID: 10, Document Name: Confessions of a Tradesman by Frank Thomas Bullen.txt\n",
            "Document ID: 11, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration.txt\n",
            "Document ID: 12, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration, v. 04 (of 10).txt\n",
            "Document ID: 13, Document Name: Business Administration- Theory, Practice and Application.txt\n",
            "Document ID: 14, Document Name: Up To Date Business by Seymour Eaton.txt\n",
            "Document ID: 15, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration, v. 01 (of 10).txt\n",
            "Document ID: 16, Document Name: Twenty Years of Hus'ling by J. P. Johnston.txt\n",
            "Document ID: 17, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration, v. 03 (of 10).txt\n",
            "Document ID: 18, Document Name: Fifty years in Wall Street by Henry Clews.txt\n"
          ]
        }
      ]
    }
  ]
}